{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb24deaa",
   "metadata": {},
   "source": [
    "# Create Knowledge Base\n",
    "\n",
    "## Notes\n",
    "\n",
    "- Check for missing dependencies (especially ones only added in the ingestion phase currently)\n",
    "- Check for multi-core processing for SBert models, seems to only use one core right now\n",
    "\n",
    "## TODO\n",
    "\n",
    "- Check results from DB without reranker vs no-reranker\n",
    "- Check token sizes / limits for Sentence Transformer / Bi-Encoder (also check used models again, mentioned parameters depend on those)\n",
    "- Check why `title` is incorrect in last crawls\n",
    "- Overall: check quality of crawled content per domain before ingesting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957c8e26",
   "metadata": {},
   "source": [
    "## Set the domain to ingest\n",
    "\n",
    "Set the domain you want to have scanned, this will include all subpages on that domain (and that domain only). Excludes links with parameters (`?`) and anchors (`#`).\n",
    "\n",
    "Also set a name that will be used to create files to persist ingested content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1f7df4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'www_kickstartds_com'\n",
    "url = 'https://www.kickstartDS.com/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf54cf",
   "metadata": {},
   "source": [
    "## Some more dependencies\n",
    "\n",
    "Install `jsonlines`, `torch` and `sentence-transformers` if not already available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199498e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install jsonlines torch sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d42004c",
   "metadata": {},
   "source": [
    "## Create knowledge base\n",
    "\n",
    "Create a knowledge base using SBert and Sentence Transformers, based on the sections of ingested pages.\n",
    "\n",
    "This is can all still be done completely \"offline\", and without any third party APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ac788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import jsonlines\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: No GPU found. Please add GPU to your notebook.\")\n",
    "    \n",
    "print('Creating SBert knowledge base.')    \n",
    "\n",
    "bi_encoder = SentenceTransformer('msmarco-distilbert-cos-v5')\n",
    "bi_encoder.max_seq_length = 256\n",
    "\n",
    "def getSectionContent(section):\n",
    "    return section['content'].replace('\\n', ' ').strip()\n",
    "\n",
    "sections = []\n",
    "with jsonlines.open('pages-' + name + '_extracted_sections.jsonl', 'r') as pages:\n",
    "    for page in pages:\n",
    "        for page_section in page['sections']:\n",
    "            section = dict()\n",
    "            section['page'] = dict()\n",
    "            section['page']['url'] = page['url']\n",
    "            section['page']['title'] = page['title']\n",
    "            section['page']['summary'] = page['summaries']['sbert']\n",
    "            section['content'] = page_section['content']['raw']\n",
    "            section['tokens'] = page_section['tokens']\n",
    "            sections.append(section)\n",
    "\n",
    "passages = []\n",
    "passages.extend(map(getSectionContent, sections))\n",
    "\n",
    "print('Passages:', len(passages))\n",
    "\n",
    "corpus_embeddings = bi_encoder.encode(passages, convert_to_tensor=True)\n",
    "\n",
    "print('Corpus embeddings created.')\n",
    "print('Corpus embedding size:', corpus_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0966c939",
   "metadata": {},
   "source": [
    "## Define (offline) search function\n",
    "\n",
    "This function will search through our corpus, and retrieve the most relevant sections in relation to the user query given as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5794facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder, util\n",
    "\n",
    "top_k = 32\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "def search(query):\n",
    "    print(\"Input question:\", query)\n",
    "\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    # Change back to .cuda() when GPU is available on Codespace\n",
    "    question_embedding = question_embedding.cpu()\n",
    "    hits = util.semantic_search(question_embedding, corpus_embeddings, top_k=top_k)\n",
    "    hits = hits[0]\n",
    "\n",
    "    cross_inp = [[query, passages[hit['corpus_id']]] for hit in hits]\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "    for idx in range(len(cross_scores)):\n",
    "        hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "    print(\"\\n-------------------------\\n\")\n",
    "    print(\"Top-5 Cross-Encoder Re-ranker hits\")\n",
    "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "    for hit in hits[0:5]:\n",
    "        print(\"\\t{:.3f}\\t{}\".format(hit['cross-score'], passages[hit['corpus_id']].replace(\"\\n\", \" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b55c40",
   "metadata": {},
   "source": [
    "## Run queries against (offline) knowledge base\n",
    "\n",
    "Using the \"search\" function defined before we can start querying our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f5a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "search(\"What is a Design System?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84646d30",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20122d8e",
   "metadata": {},
   "source": [
    "## Install DB dependencies\n",
    "\n",
    "We'll need `psycopg` for the connection, and `pgvector` to register the vector type for our embeddings to the PostgreSQL connection.\n",
    "\n",
    "We also use `python-dotenv` to load our environment, namely the `DB_PASS` for the database connection. Ensure settings this variable in your environment, or add a `.env` file next to the notebook where the variable is defined to avoid putting it into your host context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c879c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install psycopg pgvector python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e0fa6b",
   "metadata": {},
   "source": [
    "## Connect to DB\n",
    "\n",
    "Establish a connection with the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84ebf0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pgvector.psycopg import register_vector\n",
    "import psycopg\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "conn_string = \"dbname=postgres user=postgres password=\" + os.getenv('DB_PASS') + \" host=db.pzdzoelitkqizxopmwfg.supabase.co port=5432\"\n",
    "conn = psycopg.connect(conn_string, row_factory=psycopg.rows.dict_row)\n",
    "conn.autocommit = True\n",
    "conn.execute('CREATE EXTENSION IF NOT EXISTS vector')\n",
    "register_vector(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b666c1",
   "metadata": {},
   "source": [
    "## Create database tables\n",
    "\n",
    "Create a table to hold all received questions and their respective response data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8927aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = False\n",
    "\n",
    "def createDbTables():\n",
    "    conn.execute('DROP TABLE IF EXISTS question_answer_sections')\n",
    "\n",
    "    conn.execute('DROP TABLE IF EXISTS sections')\n",
    "    conn.execute('CREATE TABLE sections (id bigserial PRIMARY KEY, created_at timestamptz, updated_at timestamptz, page_url text, page_title text, page_summary text, tokens integer, content text, embedding vector(768)), domain text')\n",
    "    conn.execute('CREATE INDEX ON sections USING ivfflat (embedding vector_cosine_ops)')\n",
    "\n",
    "    conn.execute('DROP TABLE IF EXISTS questions')\n",
    "    conn.execute('CREATE TABLE questions (id bigserial PRIMARY KEY, created_at timestamptz, updated_at timestamptz, question text, prompt text, prompt_length integer, answer text, embedding vector(768))')\n",
    "    conn.execute('CREATE INDEX ON questions USING ivfflat (embedding vector_cosine_ops)')\n",
    "\n",
    "    conn.execute('CREATE TABLE question_answer_sections (question_id bigserial REFERENCES questions, section_id bigserial REFERENCES sections, similarity real, PRIMARY KEY (question_id, section_id))')\n",
    "\n",
    "if seed:\n",
    "    createDbTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9fb5ea",
   "metadata": {},
   "source": [
    "## Write embeddings to DB\n",
    "\n",
    "Write all page embeddings into PostgreSQL / pgvector / Supabase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d5ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "timestamp = datetime.now()\n",
    "\n",
    "# Change back to .cuda() when GPU is available on Codespace\n",
    "section_embeddings = corpus_embeddings.cpu()\n",
    "\n",
    "for index, page_embedding in enumerate(section_embeddings.detach().numpy()):\n",
    "    conn.execute(\"\"\"\n",
    "        INSERT INTO sections (created_at, updated_at, page_url, page_title, page_summary, tokens, content, embedding)\n",
    "        VALUES (%(created_at)s, %(updated_at)s, %(page_url)s, %(page_title)s, %(page_summary)s, %(tokens)s, %(content)s, %(embedding)s);\n",
    "    \"\"\", ({\n",
    "        'created_at': timestamp,\n",
    "        'updated_at': timestamp,\n",
    "        'page_url': sections[index]['page']['url'],\n",
    "        'page_title': sections[index]['page']['title'],\n",
    "        'page_summary': sections[index]['page']['summary'],\n",
    "        'content': sections[index]['content'],\n",
    "        'tokens': sections[index]['tokens'],\n",
    "        'embedding': page_embedding,\n",
    "        'domain': urlparse(sections[index]['page']['url']).netloc\n",
    "    }))\n",
    "\n",
    "conn.execute('REINDEX TABLE sections')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a8c7d8",
   "metadata": {},
   "source": [
    "## Define (online) search function\n",
    "\n",
    "This function can be used to run a search against the PostgreSQL vector table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62e5543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder, SentenceTransformer\n",
    "\n",
    "bi_encoder = SentenceTransformer('msmarco-distilbert-cos-v5')\n",
    "bi_encoder.max_seq_length = 256\n",
    "\n",
    "top_k = 32\n",
    "cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "\n",
    "def search(query):\n",
    "    print(\"Input question:\", query)\n",
    "\n",
    "    question_embedding = bi_encoder.encode(query, convert_to_tensor=True)\n",
    "    # Change back to .cuda() when GPU is available on Codespace\n",
    "    question_embedding = question_embedding.cpu()\n",
    "\n",
    "    hits = conn.execute('SELECT * FROM sections ORDER BY embedding <-> %s LIMIT ' + str(top_k), (question_embedding.detach().numpy(),)).fetchall()\n",
    "\n",
    "    cross_inp = [[query, hit['content']] for hit in hits]\n",
    "    cross_scores = cross_encoder.predict(cross_inp)\n",
    "\n",
    "    for idx in range(len(cross_scores)):\n",
    "        hits[idx]['cross-score'] = cross_scores[idx]\n",
    "\n",
    "    print(\"\\n-------------------------\\n\")\n",
    "    print(\"Top-5 Cross-Encoder Re-ranker hits from PostgreSQL\")\n",
    "    hits = sorted(hits, key=lambda x: x['cross-score'], reverse=True)\n",
    "    for hit in hits[0:5]:\n",
    "        print(\"\\t{:.3f}\\t{}\\t{}\".format(hit['cross-score'], hit['heading'], hit['content']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be18f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "search('What are the 5 main benefits of using Design Token in your Design System?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f17f196",
   "metadata": {},
   "source": [
    "## Call edge function with cURL locally\n",
    "\n",
    "This calls the edge function using cURL. Edge function is served locally by first using `yarn supabase start && yarn supabase functions serve`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a81932",
   "metadata": {},
   "source": [
    "```\n",
    "curl --request POST 'http://localhost:54321/functions/v1/answer' \\\n",
    "  --header 'Authorization: Bearer ADD_YOUR_TOKEN' \\\n",
    "  --header 'Content-Type: application/json' \\\n",
    "  --data '{ \"question\":\"What is the definition of a Design System?\" }'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92430c6",
   "metadata": {},
   "source": [
    "## Close DB connection\n",
    "\n",
    "After we're done, we'll close down the connection to PostgreSQL again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8655628",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
