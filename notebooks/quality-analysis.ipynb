{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from umap import UMAP\n",
    "from urllib.parse import urlparse\n",
    "from nltk.corpus import brown\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_location = '../page-embeddings.pkl'\n",
    "\n",
    "print(\"Loading pre-computed embeddings from disc: \" + embeddings_location)\n",
    "with open(embeddings_location, \"rb\") as reader:\n",
    "    cache_data = pickle.load(reader)\n",
    "    sections = cache_data['sections']\n",
    "    corpus_embeddings = cache_data['embeddings']\n",
    "\n",
    "print('Corpus embeddings loaded.')\n",
    "print('Corpus embedding size:', corpus_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: No GPU found. Please add GPU to your notebook.\")\n",
    "\n",
    "paras = brown.paras()\n",
    "\n",
    "bi_encoder = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "bi_encoder.max_seq_length = 256\n",
    "\n",
    "def joinPara(para):\n",
    "    return nltk.TreebankWordDetokenizer().detokenize(para[0])\n",
    "\n",
    "passages = []\n",
    "passages.extend(map(joinPara, np.asarray(paras, dtype=object)))\n",
    "\n",
    "print('Passages:', len(passages))\n",
    "\n",
    "quality_embeddings = bi_encoder.encode(passages, convert_to_tensor=True)\n",
    "\n",
    "print('Quality embeddings created.')\n",
    "print('Quality embedding size:', quality_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_sims = util.cos_sim(corpus_embeddings, quality_embeddings.cpu())\n",
    "print('Number of similarities computed:', len(cos_sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_fit = UMAP(n_components=3, n_neighbors=8, random_state=42)\n",
    "embeddings_umap = umap_fit.fit_transform(corpus_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/tmp/embeddings_analysis.pkl', \"wb\") as writer:\n",
    "    pickle.dump({'sections': sections, 'embeddings': corpus_embeddings, 'umap': embeddings_umap, 'similarities': cos_sims}, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(pd.json_normalize(sections, sep='_'))\n",
    "df['score'] = list(map(lambda x: x.mean(), cos_sims.numpy()))\n",
    "df['content'].apply(lambda x: x.replace('\\n', ' ').strip())\n",
    "df['domain'] = df['page_url'].apply(lambda x: urlparse(x).netloc)\n",
    "df['x'] = embeddings_umap[:, 0]\n",
    "df['y'] = embeddings_umap[:, 1]\n",
    "df['z'] = embeddings_umap[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.percentile(df['score'], 2.5)\n",
    "\n",
    "plt_range = [df['score'].min(), df['score'].max()]\n",
    "\n",
    "counts, bins = np.histogram(df['score'], range=plt_range, bins=50)\n",
    "diagram = plt.stairs(counts, bins)\n",
    "plt.axvline(threshold, color='r', linestyle='--')\n",
    "\n",
    "print('Threshold:', threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['partition'] = df['score'].apply(lambda x: x > threshold and x < (threshold * -1))\n",
    "\n",
    "fig_3d_tokens = px.scatter_3d(\n",
    "    df, x='x', y='y', z='z',\n",
    "    color=df['tokens'], hover_data=['content'], color_continuous_scale=px.colors.sequential.Viridis,\n",
    "    width=800, height=800\n",
    ")\n",
    "fig_3d_tokens.update_traces(marker_size=5)\n",
    "fig_3d_tokens.show()\n",
    "\n",
    "fig_3d_score = px.scatter_3d(\n",
    "    df, x='x', y='y', z='z',\n",
    "    color=df['score'], hover_data=['content'], color_continuous_scale=px.colors.sequential.Viridis,\n",
    "    width=800, height=800\n",
    ")\n",
    "fig_3d_score.update_traces(marker_size=5)\n",
    "fig_3d_score.show()\n",
    "\n",
    "fig_3d_partition = px.scatter_3d(\n",
    "    df, x='x', y='y', z='z',\n",
    "    color=df['partition'], hover_data=['content'], color_continuous_scale=px.colors.sequential.Viridis,\n",
    "    width=800, height=800\n",
    ")\n",
    "fig_3d_partition.update_traces(marker_size=5)\n",
    "fig_3d_partition.show()\n",
    "\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoresFrame = df['score']\n",
    "sorted_ids = scoresFrame.argsort()\n",
    "scores = scoresFrame[sorted_ids]\n",
    "\n",
    "print('Threshold:', threshold)\n",
    "\n",
    "numpy_sections = np.array(sections)\n",
    "\n",
    "good_ids = sorted_ids[np.logical_and(threshold <= scores, scores <= (threshold * -1))]\n",
    "bad_ids = sorted_ids[np.logical_or(threshold > scores, scores > (threshold * -1))]\n",
    "\n",
    "print(len(good_ids), len(bad_ids))\n",
    "\n",
    "selected_sections = numpy_sections[good_ids[0:10]]\n",
    "selected_scores = scores[good_ids[0:10]]\n",
    "\n",
    "df = pd.DataFrame.from_records(pd.json_normalize(selected_sections, sep='_'))\n",
    "df['score'] = selected_scores\n",
    "df['content'].apply(lambda x: x.replace('\\n', ' ').strip())\n",
    "df['domain'] = df['page_url'].apply(lambda x: urlparse(x).netloc)\n",
    "\n",
    "df.sample(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
