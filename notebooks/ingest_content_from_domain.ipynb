{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b6ad454",
   "metadata": {},
   "source": [
    "# Ingest Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707db4ee",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "Collection of notes to help ingest content from a domain. Includes specific instructions to filter output for some domains (like bradfrost.com).\n",
    "\n",
    "### bradfrost.com\n",
    "\n",
    "- Exclude `/blog/link/*` and `/blog/tag/*` and `/attachment` and `/blog/page/*`\n",
    "\n",
    "### danmall.com\n",
    "\n",
    "- Exclude `/topics/*`\n",
    "\n",
    "### primer.style\n",
    "\n",
    "- Exclude `/octicons/*` and `/design/foundations/icons/*` and `/view-components/lookbook/*`\n",
    "\n",
    "### digitaldesign.vattenfall.com\n",
    "\n",
    "- Exclude `/components/modules/*`.\n",
    "\n",
    "### volkswagen.frontify.com/d/rzB71PwpjXgt/\n",
    "\n",
    "- Exclude `/builder/groupui/*`.\n",
    "\n",
    "Missig a bunch of content because it's \"hidden\" behind anchor link navigation (like: https://volkswagen.frontify.com/d/rzB71PwpjXgt/documentation#/developing/plain-javascript-integration)\n",
    "\n",
    "### designstrategy.guide\n",
    "\n",
    "- Exclude `/tag/*`\n",
    "\n",
    "### www.designbetter.co\n",
    "\n",
    "- Exclude `pencils-before-pixels`\n",
    "\n",
    "### www.duetds.com\n",
    "\n",
    "- Exlucde `/components/examples/`\n",
    "\n",
    "### clarity.design\n",
    "\n",
    "- Exclude `/news/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcd0776",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "Install the following dependencies first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57cb64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bs4 jsonlines pytest-playwright"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5394c9c6",
   "metadata": {},
   "source": [
    "## Install needed browsers\n",
    "\n",
    "In this case we'll just use Chrome... but Webkit and Firefox get installed, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547fa0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!playwright install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957c8e26",
   "metadata": {},
   "source": [
    "## Set the domain to ingest\n",
    "\n",
    "Set the domain you want to have scanned, this will include all subpages on that domain (and that domain only). Excludes links with parameters (`?`) and anchors (`#`).\n",
    "\n",
    "Also set a name that will be used to create files to persist ingested content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1f7df4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'www_kickstartds_com'\n",
    "url = 'https://www.kickstartDS.com/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7023a00d",
   "metadata": {},
   "source": [
    "Think about ways of including a script / snippet per domain to handle edge cases. Some kind of hook?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d075443",
   "metadata": {},
   "source": [
    "## Find all internal URLs\n",
    "\n",
    "First step is to crawl a domain for all internal links leading to HTML content. Do this until everything is discovered. Enter your domain through adjusting the `url` in `__main__`.\n",
    "\n",
    "Write the set of discovered URLs from `all_links` to disk, converting them to `jsonl` format for easier processing in the next steps. We'll build upon that `page` dict in the following steps.\n",
    "\n",
    "Scroll delay (currently 5 * .5s) serves as a kind of \"backoff\" as to not become blocked for DDoS / obvious scraping for the moment.\n",
    "\n",
    "TODO:\n",
    "- Still some duplicate URLs (/ vs non-/)\n",
    "- Not finding some URLs when behind tabbed content? (e.g. https://atlassian.design/components/button/examples -> Usage tab)\n",
    "- Add exclude list / filter to processed urls (e.g. to exclude stuff like \"blog/tag/*\")\n",
    "- Attempt to handle Cookies / Cookie Consent, try to close cookie banners? (e.g. for screenshots)\n",
    "- Some types of links seem still to fail (e.g. \"https://danmall.com/topics/artificial intelligence\", as logged)\n",
    "- Some links (probably because of weird endless scrolling) seem to always time out, like this one: https://www.designbetter.co/principles-of-product-design/break-black-box\n",
    "- Subdomains seem to have problems (like here: https://design-system.service.gov.uk/ / https://hds.hel.fi/)\n",
    "- Sometimes empy URLs\n",
    "- Timeout on https://primer.style/design/foundations/css-utilities/animations\n",
    "- Retry-Logic and incremental crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aee3df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import jsonlines\n",
    "import pathlib\n",
    "import time\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright\n",
    "from slugify import slugify\n",
    " \n",
    "def get_domain(url):\n",
    "    parsed_uri = urlparse(url)\n",
    "    domain = '{uri.scheme}://{uri.netloc}/'.format(uri=parsed_uri)\n",
    "    return domain\n",
    "\n",
    "def get_path(url):\n",
    "    parsed_uri = urlparse(url)\n",
    "    return parsed_uri.path\n",
    " \n",
    "def get_links(url, content):\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    domain = get_domain(url)\n",
    "    links = set()\n",
    "    for link in soup.find_all('a'):\n",
    "        link_url = link.get('href')\n",
    "        if link_url:\n",
    "            absolute_link = urljoin(url, link_url)\n",
    "            if absolute_link.startswith(domain):\n",
    "                links.add(absolute_link)\n",
    "    return links\n",
    " \n",
    "async def playwright(url, slug):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch()\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url)\n",
    "        \n",
    "        screenshot_dir = 'screenshots/' + name\n",
    "        pathlib.Path(screenshot_dir).mkdir(parents=True, exist_ok=True)\n",
    "        await page.screenshot(path=f'{screenshot_dir}/{slug}-{p.chromium.name}.png')\n",
    "        \n",
    "        for i in range(5):\n",
    "            await page.mouse.wheel(0, 15000)\n",
    "            time.sleep(0.500)\n",
    "            i += 1\n",
    "        \n",
    "        content = await page.content()\n",
    "        await browser.close()\n",
    "        return content\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    queue = [url]\n",
    "    visited = set()\n",
    "    all_links = set()\n",
    "    pages = []\n",
    " \n",
    "    while queue:\n",
    "        url = queue.pop(0)\n",
    "        visited.add(url)\n",
    "        response = requests.get(url)\n",
    "        if (response.ok and \"text/html\" in response.headers['Content-Type']):\n",
    "            page = dict()\n",
    "            print(url)\n",
    "            if (url.rstrip(\"/\") not in all_links):\n",
    "                all_links.add(url.rstrip(\"/\"))\n",
    "                page['url'] = url\n",
    "                page['slug'] = slugify(get_path(page['url']))\n",
    "                page['content'] = dict()\n",
    "                page['content']['html'] = await playwright(page['url'], page['slug'])\n",
    "                pages.append(page)\n",
    "    \n",
    "                links = get_links(page['url'], page['content']['html'])\n",
    "                for link in links:\n",
    "                    if link not in visited and link not in queue and '#' not in link and '?' not in link and '/news/' not in link:\n",
    "                        queue.append(link)\n",
    "    \n",
    "    print()\n",
    "    print('All done! ' + str(len(all_links)) + ' links discovered.')\n",
    "\n",
    "    with jsonlines.open('pages-' + name + '.jsonl', 'w') as writer:\n",
    "        writer.write_all(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422625f5",
   "metadata": {},
   "source": [
    "## More dependencies\n",
    "\n",
    "Install trafilatura, that will be used to extract the content from pages, and tiktoken to have a first relevant token measurement for complete page content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d2d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install trafilatura tiktoken markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f45a7ec",
   "metadata": {},
   "source": [
    "## Extract content from discovered pages\n",
    "\n",
    "We'll keep Markdown formatting for now. It will be used to split sections from pages by slicing by headlines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b35361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jsonlines\n",
    "import tiktoken\n",
    "from bs4 import BeautifulSoup\n",
    "from trafilatura import load_html, extract\n",
    "from markdown import markdown\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
    "\n",
    "def markdown_to_text(markdown_string):\n",
    "    html = markdown(markdown_string)\n",
    "\n",
    "    html = re.sub(r'<pre>(.*?)</pre>', ' ', html)\n",
    "    html = re.sub(r'<code>(.*?)</code >', ' ', html)\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = ''.join(soup.findAll(text=True))\n",
    "\n",
    "    return text\n",
    "\n",
    "extracted_content = []\n",
    "with jsonlines.open('pages-' + name + '.jsonl') as pages:\n",
    "    for page in pages:\n",
    "        if (page['content']['html']):\n",
    "            downloaded = load_html(page['content']['html'])\n",
    "            parsed = BeautifulSoup(page['content']['html'], \"html.parser\")\n",
    "            ogTitleTag = parsed.find(\"meta\", property=\"og:title\")\n",
    "            title = parsed.title.string if (parsed.title and parsed.title.string) else ogTitleTag.get(\"content\") if (ogTitleTag and ogTitleTag.get(\"content\") != None) else page['url']\n",
    "            ogDescriptionTag = parsed.find(\"meta\", property=\"og:description\")\n",
    "            metaDescriptionTag = parsed.find(\"meta\", attrs={'name': 'description'})\n",
    "            metaDescription = metaDescriptionTag['content'] if (metaDescriptionTag and metaDescriptionTag.get(\"content\") != None) else (ogDescriptionTag.get(\"content\") if (ogDescriptionTag and ogDescriptionTag.get(\"content\") != None) else 'No meta description extractable')\n",
    "\n",
    "            result = extract(downloaded, url=page['url'], include_formatting=True, include_links=True, favor_recall=True)\n",
    "\n",
    "            if result is None:\n",
    "                print('couldnt extract:', page['url'])\n",
    "            else:\n",
    "                augmented = dict()\n",
    "                augmented['url'] = page['url']\n",
    "                augmented['slug'] = page['slug']\n",
    "\n",
    "                augmented['content'] = page['content']\n",
    "                augmented['content']['raw'] = markdown_to_text(result)\n",
    "                augmented['content']['markdown'] = result\n",
    "\n",
    "                augmented['title'] = title.replace('\\n', ' ').strip()\n",
    "                augmented['description'] = metaDescription.replace('\\n', ' ').strip()\n",
    "\n",
    "                augmented['lines'] = result.splitlines()\n",
    "                augmented['size'] = len(result)\n",
    "                augmented['token'] = len(enc.encode(result))\n",
    "\n",
    "                extracted_content.append(augmented)\n",
    "                print('extracted:', augmented['url'], augmented['title'], str(augmented['token']) + ' Token,', len(result))  \n",
    "\n",
    "with jsonlines.open('pages-' + name + '_extracted.jsonl', 'w') as pages:\n",
    "    pages.write_all(extracted_content)\n",
    "    \n",
    "print()\n",
    "print('wrote extracted content to \"pages-' + name + '_extracted.jsonl\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ce1f33",
   "metadata": {},
   "source": [
    "## Even more dependencies\n",
    "\n",
    "Install the BERT extractive summarizer and Sentence Transformers, we'll use these to create summaries as a first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912c959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bert-extractive-summarizer sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8588092c",
   "metadata": {},
   "source": [
    "## Create SBert summaries\n",
    "\n",
    "We first create SBert summaries by identifying the most central sentences on a page, concatenating those for a rough first summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89606e20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import jsonlines\n",
    "from summarizer.sbert import SBertSummarizer\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
    "model = SBertSummarizer('paraphrase-multilingual-MiniLM-L12-v2')  \n",
    "\n",
    "with_summaries = []\n",
    "with jsonlines.open('pages-' + name + '_extracted.jsonl', 'r') as pages:\n",
    "    for page in pages:\n",
    "        with_summaries.append(page)\n",
    "    \n",
    "print('Creating summaries for ' + str(len(list(with_summaries))) + ' pages.')\n",
    "print()\n",
    "\n",
    "for page in with_summaries:\n",
    "    result = model(page['content']['raw'].replace('\\n', ' '), num_sentences=4, min_length=60)\n",
    "    metaDescription = page.pop('description')\n",
    "    page['summaries'] = dict()\n",
    "    page['summaries']['sbert'] = ''.join(result)\n",
    "    page['summaries']['meta'] = metaDescription\n",
    "    print(page['url'], page['title'], str(len(enc.encode(page['summaries']['sbert']))) + ' Token,')\n",
    "        \n",
    "with jsonlines.open('pages-' + name + '_extracted_summaries.jsonl', 'w') as pages:\n",
    "    pages.write_all(with_summaries)\n",
    "\n",
    "print()\n",
    "print('All summaries created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac588550",
   "metadata": {},
   "source": [
    "## Install dependencies for splitting\n",
    "\n",
    "We'll split along Markdown syntax, using `langchain` to split as intelligently as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fd181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain markdownify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c46e23c",
   "metadata": {},
   "source": [
    "## Extract sections from markdown page content\n",
    "\n",
    "We'll extract sections from our pages by splitting along markdown headlines (# to ######)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc8ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import tiktoken\n",
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "from markdownify import MarkdownConverter\n",
    "from markdown import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
    "markdown_splitter = MarkdownTextSplitter.from_tiktoken_encoder(chunk_size=300, chunk_overlap=50)\n",
    "\n",
    "class IgnorantConverter(MarkdownConverter):\n",
    "    def convert_script(self, el, text, convert_as_inline):\n",
    "        return ''\n",
    "    \n",
    "    def convert_iframe(self, el, text, convert_as_inline):\n",
    "        return ''\n",
    "    \n",
    "    def convert_style(self, el, text, convert_as_inline):\n",
    "        return ''\n",
    "\n",
    "def md(html, **options):\n",
    "    return IgnorantConverter(**options).convert(html)\n",
    "\n",
    "pages = []\n",
    "with jsonlines.open('pages-' + name + '_extracted_summaries.jsonl') as pagesReader:\n",
    "    pages = [page for page in pagesReader]\n",
    "    sections = markdown_splitter.create_documents(\n",
    "        list(map(lambda page: md(\n",
    "            page[\"content\"][\"html\"],\n",
    "            convert=[\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\", \"script\", \"style\", \"iframe\", \"b\", \"ul\", \"li\", \"a\", \"ol\", \"quote\"],\n",
    "            heading_style=\"ATX\"\n",
    "        ), pages)),\n",
    "        list(map(lambda page: dict((k, page[k]) for k in ['url'] if k in page), pages))\n",
    "    )\n",
    "\n",
    "    for page in pages:\n",
    "        page['sections'] = []\n",
    "\n",
    "    for section in sections:\n",
    "        pageIndex = [index for (index, item) in enumerate(pages) if item['url'] == section.metadata['url']][0]\n",
    "        \n",
    "        if (not \"sections\" in pages[pageIndex]):\n",
    "            pages[pageIndex][\"sections\"] = []\n",
    "\n",
    "        pageSection = dict()\n",
    "        pageSection['content'] = dict()\n",
    "        pageSection['content']['markdown'] = section.page_content\n",
    "        pageSection['content']['raw'] = ''.join(BeautifulSoup(markdown(section.page_content)).findAll(text=True))\n",
    "        pageSection['tokens'] = len(enc.encode(section.page_content))\n",
    "\n",
    "        pages[pageIndex][\"sections\"].append(pageSection)\n",
    "\n",
    "with jsonlines.open('pages-' + name + '_extracted_sections.jsonl', 'w') as pagesWriter:\n",
    "    pagesWriter.write_all(pages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
